{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SVM model.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yfo0cpigFrSb"
      },
      "source": [
        "## **SVM Full Name Model**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "avBvoeR2GkoG"
      },
      "source": [
        "In this we have create support vector classification model using full name as token. The support vector machine is a type of algorithm determining the boundary between vector \n",
        "belonging to specific class and vectors not belonging to it.The best-fitting line is called \n",
        "hyperplane, which divides the space into a given number of planes.SVM algorithm is implemented using kernel. The kernel changes a input data \n",
        "space into required form.It take two data point (a and b) and \n",
        "calculate a distance score. The closer the point are higher the distance. The data points are them \n",
        "mapped in a kernel function (Ï†((a, b)) = (a, b, a2 + b2)) . This helps in transforming the \n",
        "data point in higher dimensional aping"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-TQKuTTOHht6"
      },
      "source": [
        "## Data processing\n",
        "\n",
        "Here We import the ethnicity data from the file and select the relevant columns. We converted labels into numeric and clean the names by removing suffixes and special characters. Cleaning data is essential as it can create wrong models.\n",
        "\n",
        "**vectorising text** : It is important part to create the token and vocabulary from the words.We have use countvectorizer. It is used to transform a given text into a vector on the basis of the frequency (count) of each word that occurs in the entire text. Basically we converted each name into a single vector.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sWoFHjq-feRh",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 190
        },
        "outputId": "874f4d04-c9c9-488f-f1a5-756ed817b291"
      },
      "source": [
        "# Importing librairies\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report,confusion_matrix,accuracy_score\n",
        "import re\n",
        "from sklearn.model_selection import KFold\n",
        "import joblib\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.svm import LinearSVC\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "# read data , and choose columns and get info about data\n",
        "df = pd.read_csv(\"ethnicity collection data11-asian specific.csv\")\n",
        "df.shape\n",
        "df.head()\n",
        "df.groupby('ethinicity')['full name'].size()\n",
        "df =df[['full name','ethinicity']]\n",
        "\n",
        "# convert the columns into lower case and remove na\n",
        "df['full name'] = df['full name'].str.lower()\n",
        "df['ethinicity']=df['ethinicity'].str.lower()\n",
        "df['ethinicity']=df['ethinicity'].map({'asian-indian':1,'black non hispanic':2,'hispanic':3 , 'white non hispanic':4,'asian-east':5})\n",
        "df = df.dropna()\n",
        "df\n",
        "# clean text, remove special charaters, numbers,suffix(Full name),  \n",
        "def cleaning(text):\n",
        "    only_words = re.sub('([^A-Za-z ]+)|(^dr\\.)|(^dr )|(^mr\\.)|(^mr )|(^prof\\.)|(^adv\\. )',' ',text )\n",
        "    return only_words\n",
        "\n",
        "df['full_name_cleaned']=df['full name'].apply(cleaning)\n",
        "df\n",
        "#save data in new file \n",
        "df.to_csv('data.csv', encoding='utf-8', index=False)\n",
        "#load new data\n",
        "cleaned_data = pd.read_csv(\"data.csv\", encoding = \"ISO-8859-1\")\n",
        "pd.set_option('display.max_colwidth', None)\n",
        "#convert lables \n",
        "data = cleaned_data.full_name_cleaned\n",
        "target = cleaned_data.ethinicity\n",
        "\n",
        "# vectorise tex - ngram 2,3,4 ,tf-idf/count vectoriser\n",
        "countvect = CountVectorizer(ngram_range=(1,2)) \n",
        "name = countvect.fit_transform(data)\n",
        "a =countvect.get_feature_names()\n",
        "len(a)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/externals/six.py:31: FutureWarning: The module is deprecated in version 0.21 and will be removed in version 0.23 since we've dropped support for Python 2.7. Please rely on the official version of six (https://pypi.org/project/six/).\n",
            "  \"(https://pypi.org/project/six/).\", FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/deprecation.py:144: FutureWarning: The sklearn.neighbors.base module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.neighbors. Anything that cannot be imported from sklearn.neighbors is now part of the private API.\n",
            "  warnings.warn(message, FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
            "  import pandas.util.testing as tm\n",
            "/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py:2718: DtypeWarning: Columns (0,1,2,5,7) have mixed types.Specify dtype option on import or set low_memory=False.\n",
            "  interactivity=interactivity, compiler=compiler, result=result)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "860086"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FSPu-YxsIYIk"
      },
      "source": [
        "##Model Creation \n"
        "Here to create a model we have done following steps :\n",
        "\n",
        "\n",
        "\n",
        "1.   We have used cross- validation for comparing the model and select the best-trained model. It presents \n",
        "the general idea of how the model will perform on unseen data. The result of cross-validation\n",
        "is generally the output of a model that is less biased or less optimistic.We specifically used k-fold technique with k = 10.\n",
        "2.   We have used SMOTE for oversampling the data. Our data was not balance which can create a problem of bias. To avoide that we used SMOTE.\n",
        "3. We split the data into train-test and ran the model.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4XyKZ3AGgT2d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "24698bce-ee07-42bb-9922-604d8bdca9d1"
      },
      "source": [
        "# model fitting and cross validation k fold - 10 \n",
        "# we have also done oversamlpling using SMOTE\n",
        "\n",
        "def Cross_validation(data, tagret, countvect, clf_cv, model_name): #Performs cross-validation \n",
        "\n",
        "    kf = KFold(n_splits=10, shuffle=True, random_state=1) # 10-fold cross-validation\n",
        "    scores=[]\n",
        "    data_train_list = []\n",
        "    targets_train_list = []\n",
        "    data_test_list = []\n",
        "    targets_test_list = []\n",
        "    iteration = 0\n",
        "    print(\"Performing cross-validation for {}...\".format(model_name))\n",
        "    for train_index, test_index in kf.split(data):\n",
        "        iteration += 1\n",
        "        print(\"Iteration \", iteration)\n",
        "\n",
        "        # Split train and text data.\n",
        "        data_train_cv, targets_train_cv = data[train_index], target[train_index] \n",
        "        data_test_cv, targets_test_cv = data[test_index], target[test_index] \n",
        "        data_train_list.append(data_train_cv) \n",
        "        data_test_list.append(data_test_cv) \n",
        "        targets_train_list.append(targets_train_cv) \n",
        "        targets_test_list.append(targets_test_cv)\n",
        "        \n",
        "        # learning vocabulary of training set\n",
        "        countvect.fit(data_train_cv.values.astype('U')) \n",
        "        data_train_countvect_cv = countvect.transform(data_train_cv.values.astype('U'))\n",
        "        print(data_train_countvect_cv.shape)\n",
        "        print(targets_train_cv.shape)\n",
        "\n",
        "        #balancing Trainign dataset for each iteration \n",
        "        print(\"Number of observations in each class before oversampling (training data): \\n\", pd.Series(targets_train_cv).value_counts())\n",
        "        smote = SMOTE(random_state = 101)\n",
        "        data_train_countvect_cv,targets_train_cv = smote.fit_sample(data_train_countvect_cv,targets_train_cv)\n",
        "        print(\"Number of observations in each class after oversampling (training data): \\n\", pd.Series(targets_train_cv).value_counts())\n",
        "\n",
        "        #print shape of train and test data\n",
        "        print(\"Shape of training data: \", data_train_countvect_cv.shape)\n",
        "        # converting test data using countvectoriser \n",
        "        data_test_countvect_cv = countvect.transform(data_test_cv.values.astype('U'))\n",
        "        print(\"Shape of test data: \", data_test_countvect_cv.shape)\n",
        "        clf_cv.fit(data_train_countvect_cv, targets_train_cv) # Fitting model\n",
        "        score = clf_cv.score(data_test_countvect_cv, targets_test_cv) # Calculating accuracy\n",
        "        scores.append(score) # appending cross-validation accuracy for each iteration\n",
        "    print(\"List of cross-validation accuracies for {}: \".format(model_name), scores)\n",
        "    mean_accuracy = np.mean(scores)\n",
        "    print(\"Mean cross-validation accuracy for {}: \".format(model_name), mean_accuracy)\n",
        "    print(\"Best cross-validation accuracy for {}: \".format(model_name), max(scores))\n",
        "\n",
        "    #finding best cross-validation for best set\n",
        "    max_acc_index = scores.index(max(scores)) #\n",
        "    max_acc_data_train = data_train_list[max_acc_index]\n",
        "    max_acc_data_test = data_test_list[max_acc_index]\n",
        "    max_acc_targets_train = targets_train_list[max_acc_index] \n",
        "    max_acc_targets_test = targets_test_list[max_acc_index] \n",
        "\n",
        "    return mean_accuracy, max_acc_data_train, max_acc_data_test, max_acc_targets_train, max_acc_targets_test\n",
        "\n",
        "def c_matrix(max_acc_data_train, max_acc_data_test, max_acc_targets_train, max_acc_targets_test, countvect, target, clf, model_name): #### Creates Confusion matrix for SVC\n",
        "    countvect.fit(max_acc_data_train.values.astype('U'))\n",
        "    max_acc_data_train_countvect = countvect.transform(max_acc_data_train.values.astype('U'))\n",
        "    max_acc_data_test_countvect = countvect.transform(max_acc_data_test.values.astype('U'))\n",
        "    clf.fit(max_acc_data_train_countvect, max_acc_targets_train) # Fitting SVC\n",
        "    targets_pred = clf.predict(max_acc_data_test_countvect) # Prediction on test data\n",
        "    #  printing confusion mattrix\n",
        "    conf_mat = classification_report(max_acc_targets_test, targets_pred)\n",
        "    print(conf_mat)\n",
        "\n",
        "# run model\n",
        "SVC_clf = LinearSVC() \n",
        "SVC_mean_accuracy, max_acc_data_train, max_acc_data_test, max_acc_targets_train, max_acc_targets_test = Cross_validation(data, target, countvect, SVC_clf, \"SVC\") # SVC cross-validation\n",
        "c_matrix(max_acc_data_train, max_acc_data_test, max_acc_targets_train, max_acc_targets_test, countvect, target, SVC_clf, \"SVC\") # SVC confusion matrix\n",
        "\n",
        "# Model saving\n",
        "def SVC_Save(data, target, countvect):\n",
        "    countvect.fit(data.values.astype('U')) # learn vocabulary of entire data\n",
        "    data_countvect = countvect.transform(data.values.astype('U'))\n",
        "    # saving vocabulary\n",
        "    pd.DataFrame.from_dict(data=dict([word, i] for i, word in enumerate(countvect.get_feature_names())), orient='index').to_csv('vocabulary_SVC.csv', header=False)\n",
        "    print(\"Shape of countvectoriser matrix for saved SVC Model: \", data_countvect.shape)\n",
        "    clf = LinearSVC().fit(data_countvect, target)\n",
        "    joblib.dump(clf, 'svc.sav')\n",
        "\n",
        "#saving model\n",
        "SVC_Save(data, target, countvect)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Performing cross-validation for SVC...\n",
            "Iteration  1\n",
            "(560904, 790226)\n",
            "(560904,)\n",
            "Number of observations in each class before oversampling (training data): \n",
            " 4.0    374214\n",
            "1.0     85482\n",
            "3.0     49811\n",
            "2.0     37902\n",
            "5.0     13495\n",
            "Name: ethinicity, dtype: int64\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
            "  warnings.warn(msg, category=FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Number of observations in each class after oversampling (training data): \n",
            " 1.0    374214\n",
            "5.0    374214\n",
            "4.0    374214\n",
            "3.0    374214\n",
            "2.0    374214\n",
            "dtype: int64\n",
            "Shape of training data:  (1871070, 790226)\n",
            "Shape of test data:  (62323, 790226)\n",
            "Iteration  2\n",
            "(560904, 789900)\n",
            "(560904,)\n",
            "Number of observations in each class before oversampling (training data): \n",
            " 4.0    374124\n",
            "1.0     85736\n",
            "3.0     49659\n",
            "2.0     37962\n",
            "5.0     13423\n",
            "Name: ethinicity, dtype: int64\n",
            "Number of observations in each class after oversampling (training data): \n",
            " 1.0    374124\n",
            "5.0    374124\n",
            "4.0    374124\n",
            "3.0    374124\n",
            "2.0    374124\n",
            "dtype: int64\n",
            "Shape of training data:  (1870620, 789900)\n",
            "Shape of test data:  (62323, 789900)\n",
            "Iteration  3\n",
            "(560904, 789837)\n",
            "(560904,)\n",
            "Number of observations in each class before oversampling (training data): \n",
            " 4.0    374394\n",
            "1.0     85395\n",
            "3.0     49696\n",
            "2.0     37972\n",
            "5.0     13447\n",
            "Name: ethinicity, dtype: int64\n",
            "Number of observations in each class after oversampling (training data): \n",
            " 1.0    374394\n",
            "5.0    374394\n",
            "4.0    374394\n",
            "3.0    374394\n",
            "2.0    374394\n",
            "dtype: int64\n",
            "Shape of training data:  (1871970, 789837)\n",
            "Shape of test data:  (62323, 789837)\n",
            "Iteration  4\n",
            "(560904, 790116)\n",
            "(560904,)\n",
            "Number of observations in each class before oversampling (training data): \n",
            " 4.0    374340\n",
            "1.0     85493\n",
            "3.0     49660\n",
            "2.0     37975\n",
            "5.0     13436\n",
            "Name: ethinicity, dtype: int64\n",
            "Number of observations in each class after oversampling (training data): \n",
            " 1.0    374340\n",
            "5.0    374340\n",
            "4.0    374340\n",
            "3.0    374340\n",
            "2.0    374340\n",
            "dtype: int64\n",
            "Shape of training data:  (1871700, 790116)\n",
            "Shape of test data:  (62323, 790116)\n",
            "Iteration  5\n",
            "(560904, 789934)\n",
            "(560904,)\n",
            "Number of observations in each class before oversampling (training data): \n",
            " 4.0    374330\n",
            "1.0     85423\n",
            "3.0     49762\n",
            "2.0     38017\n",
            "5.0     13372\n",
            "Name: ethinicity, dtype: int64\n",
            "Number of observations in each class after oversampling (training data): \n",
            " 1.0    374330\n",
            "5.0    374330\n",
            "4.0    374330\n",
            "3.0    374330\n",
            "2.0    374330\n",
            "dtype: int64\n",
            "Shape of training data:  (1871650, 789934)\n",
            "Shape of test data:  (62323, 789934)\n",
            "Iteration  6\n",
            "(560904, 789856)\n",
            "(560904,)\n",
            "Number of observations in each class before oversampling (training data): \n",
            " 4.0    374281\n",
            "1.0     85461\n",
            "3.0     49745\n",
            "2.0     37939\n",
            "5.0     13478\n",
            "Name: ethinicity, dtype: int64\n",
            "Number of observations in each class after oversampling (training data): \n",
            " 1.0    374281\n",
            "5.0    374281\n",
            "4.0    374281\n",
            "3.0    374281\n",
            "2.0    374281\n",
            "dtype: int64\n",
            "Shape of training data:  (1871405, 789856)\n",
            "Shape of test data:  (62323, 789856)\n",
            "Iteration  7\n",
            "(560904, 790034)\n",
            "(560904,)\n",
            "Number of observations in each class before oversampling (training data): \n",
            " 4.0    374345\n",
            "1.0     85521\n",
            "3.0     49681\n",
            "2.0     37927\n",
            "5.0     13430\n",
            "Name: ethinicity, dtype: int64\n",
            "Number of observations in each class after oversampling (training data): \n",
            " 1.0    374345\n",
            "5.0    374345\n",
            "4.0    374345\n",
            "3.0    374345\n",
            "2.0    374345\n",
            "dtype: int64\n",
            "Shape of training data:  (1871725, 790034)\n",
            "Shape of test data:  (62323, 790034)\n",
            "Iteration  8\n",
            "(560905, 790009)\n",
            "(560905,)\n",
            "Number of observations in each class before oversampling (training data): \n",
            " 4.0    374298\n",
            "1.0     85489\n",
            "3.0     49693\n",
            "2.0     37965\n",
            "5.0     13460\n",
            "Name: ethinicity, dtype: int64\n",
            "Number of observations in each class after oversampling (training data): \n",
            " 1.0    374298\n",
            "5.0    374298\n",
            "4.0    374298\n",
            "3.0    374298\n",
            "2.0    374298\n",
            "dtype: int64\n",
            "Shape of training data:  (1871490, 790009)\n",
            "Shape of test data:  (62322, 790009)\n",
            "Iteration  9\n",
            "(560905, 789874)\n",
            "(560905,)\n",
            "Number of observations in each class before oversampling (training data): \n",
            " 4.0    374346\n",
            "1.0     85460\n",
            "3.0     49720\n",
            "2.0     37950\n",
            "5.0     13429\n",
            "Name: ethinicity, dtype: int64\n",
            "Number of observations in each class after oversampling (training data): \n",
            " 1.0    374346\n",
            "5.0    374346\n",
            "4.0    374346\n",
            "3.0    374346\n",
            "2.0    374346\n",
            "dtype: int64\n",
            "Shape of training data:  (1871730, 789874)\n",
            "Shape of test data:  (62322, 789874)\n",
            "Iteration  10\n",
            "(560905, 790302)\n",
            "(560905,)\n",
            "Number of observations in each class before oversampling (training data): \n",
            " 4.0    374248\n",
            "1.0     85486\n",
            "3.0     49715\n",
            "2.0     37975\n",
            "5.0     13481\n",
            "Name: ethinicity, dtype: int64\n",
            "Number of observations in each class after oversampling (training data): \n",
            " 1.0    374248\n",
            "5.0    374248\n",
            "4.0    374248\n",
            "3.0    374248\n",
            "2.0    374248\n",
            "dtype: int64\n",
            "Shape of training data:  (1871240, 790302)\n",
            "Shape of test data:  (62322, 790302)\n",
            "List of cross-validation accuracies for SVC:  [0.8241740609405838, 0.8131187523065321, 0.8246233332798485, 0.822056062769764, 0.8231631981772379, 0.8217030630746274, 0.8256502414838823, 0.8222297102146915, 0.8244119251628639, 0.8226308526683996]\n",
            "Mean cross-validation accuracy for SVC:  0.8223761200078432\n",
            "Best cross-validation accuracy for SVC:  0.8256502414838823\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         1.0       0.93      0.98      0.96      9473\n",
            "         2.0       0.70      0.37      0.48      4249\n",
            "         3.0       0.83      0.73      0.78      5557\n",
            "         4.0       0.91      0.96      0.94     41535\n",
            "         5.0       0.92      0.76      0.83      1509\n",
            "\n",
            "    accuracy                           0.90     62323\n",
            "   macro avg       0.86      0.76      0.80     62323\n",
            "weighted avg       0.89      0.90      0.89     62323\n",
            "\n",
            "Shape of tfidf matrix for saved SVC Model:  (623227, 860086)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NwKD6mTsJlM0"
      },
      "source": [
        "Our model was created using vocabulary of 790302 words. We observed here that our model give us accuracy of 0.90 with recall score of 0.98 for indian asian ethnicity. We saved that model and vocabulary for further use."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M5e3iYvIKUE4"
      },
      "source": [
        "# Deploying model\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FMYUY1qtKmDo"
      },
      "source": [
        "To deploy this model on unseen data of the company, we import the data and the saved model and vocabulary. We cleaned the data and removed unwanted data that is not classified as human names (e.g., LLC, trust, ltd .etc). We also removed suffixes and special characters to get a better result. We ran the model on unseen data and checked the results."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3xC4HW8FKdSO"
      },
      "source": [
        "# DEPLOYMENT OF SVC MODEL ON COMPANY DATA\n",
        "\n",
        "#Import libraries\n",
        "import re, nltk\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import csv\n",
        "from sklearn.externals import joblib\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# Load data and info, select the relevant column\n",
        "df = pd.read_csv(DCourse BAresearchTest datamain1.csv)\n",
        "df.shape\n",
        "df.head()\n",
        "df =df[['prop_owner_name', 'IsSelectedforExport']]\n",
        "\n",
        "# convert the columns into lower case and remove na\n",
        "df['prop_owner_name'] = df['prop_owner_name'].str.lower()\n",
        "df = df.dropna()\n",
        "# cleaning unwanted data\n",
        "df = df[~(df.prop_owner_name.str.contains('llc'))]\n",
        "df = df[~(df.prop_owner_name.str.contains('ltd'))]\n",
        "df = df[~(df.prop_owner_name.str.contains('estate'))]\n",
        "df = df[~(df.prop_owner_name.str.contains('trust'))]\n",
        "\n",
        "# divide two name into seperate column\n",
        "new=df['prop_owner_name'].str.split(&,n = 1, expand = True)\n",
        "df['name1']= new[0]\n",
        "df['name2']= new[1]\n",
        "\n",
        " \n",
        "# clean text, remove special charaters, numbers,suffix(Full name),  \n",
        "def cleaning(text)\n",
        "    only_words = re.sub('([^A-Za-z ]+)|(^dr.)|(^dr )|(^mr.)|(^mr )|(^prof.)|(^adv. )',' ',text )\n",
        "    return only_words\n",
        "\n",
        "df['name1']=df['name1'].apply(cleaning)\n",
        "\n",
        "# loading model and vocabulary\n",
        "model = joblib.load('svc.sav')\n",
        "vocabulary_model = pd.read_csv('vocabulary_SVC.csv', header=None)\n",
        "\n",
        "#converting vocabulary in dictionary\n",
        "vocabulary_model_dict = {}\n",
        "for i, word in enumerate(vocabulary_model[0])\n",
        "   vocabulary_model_dict[word] = i\n",
        "countvect = CountVectorizer(ngram_range=(1,2),vocabulary = vocabulary_model_dict) \n",
        "\n",
        "#coberting new names into countvector\n",
        "new_name_countvect = countvect.fit_transform(df['name1'])\n",
        "\n",
        "#applying model and prediciting\n",
        "targets_pred = model.predict(new_name_countvect)\n",
        "df['predicited_ethnicity'] = targets_pred\n",
        "\n",
        "# saving prediction to new file\n",
        "df.to_csv('predicted_name.csv', encoding='utf-8', index=False)\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
