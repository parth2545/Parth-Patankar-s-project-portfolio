{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NBC Name model.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R-0_m6lRRNlO"
      },
      "source": [
        "#** NBC Full name model**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k4n2wE0uRb3i"
      },
      "source": [
        "The naïve Bayesian approach developed from classical \n",
        "mathematical theory and has a sound mathematical base and consistent classification \n",
        "usefulness. Naive Bayes is based on Bayes probability theorem. It is a simple classifier that \n",
        "works on the probability of events. Encoding this probability is extremely helpful as, in later \n",
        "cases, it adds on to give us the final probability of a name. However, the main disadvantage of \n",
        "using Naïve Bayes probability classifier it assigns “0” probability for the words not in the list. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qz91nUk0RsrD"
      },
      "source": [
        "## Data processing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IVieXsbnRxz3"
      },
      "source": [
        "Here We import the ethnicity data from the file and select the relevant columns. We converted labels into numeric and clean the names by removing suffixes and special characters. Cleaning data is essential as it can create wrong models.\n",
        "\n",
        "vectorising text : It is important part to create the token and vocabulary from the words.We have use countvectorizer. It is used to transform a given text into a vector on the basis of the frequency (count) of each word that occurs in the entire text. Basically we converted each name into a single vector."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6lP-Faw5wfNA",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "96c40057-35f7-4022-e991-80e82185e39d"
      },
      "source": [
        "# NBC FULL NAME MODEL\n",
        "\n",
        "#import libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.metrics import classification_report,confusion_matrix,accuracy_score\n",
        "import re\n",
        "from sklearn.model_selection import KFold\n",
        "import joblib\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# read data , and choose columns and get info about data\n",
        "df = pd.read_csv(\"ethnicity collection data.csv\")\n",
        "df.shape\n",
        "df.head()\n",
        "df.groupby('ethinicity')['full name'].size()\n",
        "df =df[['full name','ethinicity']]\n",
        "\n",
        "# convert the columns into lower case and remove na\n",
        "# converting classes into numeric \n",
        "df['full name'] = df['full name'].str.lower()\n",
        "df['ethinicity']=df['ethinicity'].str.lower()\n",
        "df['ethinicity']=df['ethinicity'].map({'asian-indian':1,'black non hispanic':2,'hispanic':3 , 'white non hispanic':4,'asian-east':5})\n",
        "df = df.dropna()\n",
        "\n",
        "# clean text, remove special charaters, numbers,suffix(Full name),  \n",
        "def cleaning(text):\n",
        "    only_words = re.sub('([^A-Za-z ]+)|(^dr\\.)|(^dr )|(^mr\\.)|(^mr )|(^prof\\.)|(^adv\\. )',' ',text )\n",
        "    return only_words\n",
        "\n",
        "df['full_name_cleaned']=df['full name'].apply(cleaning)\n",
        "\n",
        "#save data in new file \n",
        "df.to_csv('data.csv', encoding='utf-8', index=False)\n",
        "#load new data\n",
        "cleaned_data = pd.read_csv(\"data.csv\", encoding = \"ISO-8859-1\")\n",
        "pd.set_option('display.max_colwidth', None)\n",
        "#convert lables \n",
        "data = cleaned_data.full_name_cleaned\n",
        "target = cleaned_data.ethinicity\n",
        "\n",
        "# vectorise text - ngram 1,2 using count vectoriser\n",
        "# here we are using full name model\n",
        "countvect = CountVectorizer(ngram_range=(1,2))\n",
        "name = countvect.fit_transform(data)\n",
        "a =countvect.get_feature_names()\n",
        "len(a)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py:2718: DtypeWarning: Columns (0,1,2,5,7) have mixed types.Specify dtype option on import or set low_memory=False.\n",
            "  interactivity=interactivity, compiler=compiler, result=result)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4xFlSbs4R_e0"
      },
      "source": [
        "## Model Creation\n",
        "\n",
        "Here to create a model we have done following steps :\n",
        "\n",
        "1.   We have used cross- validation for comparing the model and select the \n",
        "best-trained model. It presents the general idea of how the model will perform on unseen data. The result of cross-validation is generally the output of a model that is less biased or less optimistic.We specifically used k-fold technique with k = 10.\n",
        "2.   We have used SMOTE for oversampling the data. Our data was not balance which can create a problem of bias. To avoide that we used SMOTE.\n",
        "3. We split the data into train-test and ran the model.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ll6X2D1UxNu3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "4b256a54-c0e7-4ea4-d855-0ed4193c9390"
      },
      "source": [
        "#implementing cross validation, oversampling and fitting model \n",
        "def Cross_validation(data, tagret, countvect, clf_cv, model_name): #Performs cross-validation \n",
        "\n",
        "    kf = KFold(n_splits=10, shuffle=True, random_state=1) # 10-fold cross-validation\n",
        "    scores=[]\n",
        "    data_train_list = []\n",
        "    targets_train_list = []\n",
        "    data_test_list = []\n",
        "    targets_test_list = []\n",
        "    iteration = 0\n",
        "    print(\"Performing cross-validation for {}...\".format(model_name))\n",
        "    for train_index, test_index in kf.split(data):\n",
        "        iteration += 1\n",
        "        print(\"Iteration \", iteration)\n",
        "        #spliting train text data\n",
        "        data_train_cv, targets_train_cv = data[train_index], target[train_index]\n",
        "        data_test_cv, targets_test_cv = data[test_index], target[test_index]\n",
        "        data_train_list.append(data_train_cv) \n",
        "        data_test_list.append(data_test_cv) \n",
        "        targets_train_list.append(targets_train_cv) \n",
        "        targets_test_list.append(targets_test_cv)\n",
        "\t# using countvectoriser to convert text into computer understanding language\n",
        "        countvect.fit(data_train_cv.values.astype('U')) # learning vocabulary of training set\n",
        "        data_train_countvect_cv = countvect.transform(data_train_cv.values.astype('U'))\n",
        "        print(data_train_countvect_cv.shape)\n",
        "        print(targets_train_cv.shape)\n",
        "\n",
        "        #balancing Trainign dataset for each itteration using SMOTE\n",
        "        print(\"Number of observations in each class before oversampling (training data): \\n\", pd.Series(targets_train_cv).value_counts())\n",
        "        smote = SMOTE(random_state = 101)\n",
        "        data_train_countvect_cv,targets_train_cv = smote.fit_sample(data_train_countvect_cv,targets_train_cv)\n",
        "        print(\"Number of observations in each class after oversampling (training data): \\n\", pd.Series(targets_train_cv).value_counts())\n",
        "        \n",
        "\t#print shape of train and test data\n",
        "        print(\"Shape of training data: \", data_train_countvect_cv.shape)\n",
        "        data_test_countvect_cv = countvect.transform(data_test_cv.values.astype('U'))\n",
        "        print(\"Shape of test data: \", data_test_countvect_cv.shape)\n",
        "        clf_cv.fit(data_train_countvect_cv, targets_train_cv) # Fitting model\n",
        "        score = clf_cv.score(data_test_countvect_cv, targets_test_cv) # Calculating accuracy\n",
        "        scores.append(score) # appending cross-validation accuracy for each iteration\n",
        "    print(\"List of cross-validation accuracies for {}: \".format(model_name), scores)\n",
        "    mean_accuracy = np.mean(scores)\n",
        "    print(\"Mean cross-validation accuracy for {}: \".format(model_name), mean_accuracy)\n",
        "    print(\"Best cross-validation accuracy for {}: \".format(model_name), max(scores))\n",
        "\n",
        "    #finding best cross-validation for best set\n",
        "    max_acc_index = scores.index(max(scores)) #\n",
        "    max_acc_data_train = data_train_list[max_acc_index]\n",
        "    max_acc_data_test = data_test_list[max_acc_index]\n",
        "    max_acc_targets_train = targets_train_list[max_acc_index] \n",
        "    max_acc_targets_test = targets_test_list[max_acc_index] \n",
        "\n",
        "    return mean_accuracy, max_acc_data_train, max_acc_data_test, max_acc_targets_train, max_acc_targets_test\n",
        "\n",
        "def c_matrix(max_acc_data_train, max_acc_data_test, max_acc_targets_train, max_acc_targets_test, countvect, target, clf, model_name): #### Creates Confusion matrix for NBC\n",
        "    countvect.fit(max_acc_data_train.values.astype('U'))\n",
        "    max_acc_data_train_countvect = countvect.transform(max_acc_data_train.values.astype('U'))\n",
        "    max_acc_data_test_countvect = countvect.transform(max_acc_data_test.values.astype('U'))\n",
        "    clf.fit(max_acc_data_train_countvect, max_acc_targets_train) # Fitting NBC\n",
        "    targets_pred = clf.predict(max_acc_data_test_countvect) # Prediction on test data\n",
        "    conf_mat = classification_report(max_acc_targets_test, targets_pred)\n",
        "    print(conf_mat)\n",
        "\n",
        "# firring model \n",
        "NBC_clf = MultinomialNB() \n",
        "NBC_mean_accuracy, max_acc_data_train, max_acc_data_test, max_acc_targets_train, max_acc_targets_test = Cross_validation(data, target, countvect, NBC_clf, \"NBC\") # NBC cross-validation\n",
        "c_matrix(max_acc_data_train, max_acc_data_test, max_acc_targets_train, max_acc_targets_test, countvect, target, NBC_clf, \"NBC\") # NBC confusion matrix\n",
        "\n",
        "# Saving model\n",
        "def NBC_Save(data, target, countvect):\n",
        "    countvect.fit(data.values.astype('U')) # learn vocabulary of entire data\n",
        "    data_countvect = countvect.transform(data.values.astype('U'))\n",
        "    pd.DataFrame.from_dict(data=dict([word, i] for i, word in enumerate(countvect.get_feature_names())), orient='index').to_csv('vocabulary_NBC.csv', header=False)\n",
        "    print(\"Shape of countvect matrix for saved NBC Model: \", data_countvect.shape)\n",
        "    clf = MultinomialNB().fit(data_countvect, target)\n",
        "    joblib.dump(clf, 'nbc.sav')\n",
        "\n",
        "NBC_Save(data, target, countvect)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Performing cross-validation for NBC...\n",
            "Iteration  1\n",
            "(560904, 790226)\n",
            "(560904,)\n",
            "Number of observations in each class before oversampling (training data): \n",
            " 4.0    374214\n",
            "1.0     85482\n",
            "3.0     49811\n",
            "2.0     37902\n",
            "5.0     13495\n",
            "Name: ethinicity, dtype: int64\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
            "  warnings.warn(msg, category=FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Number of observations in each class after oversampling (training data): \n",
            " 1.0    374214\n",
            "5.0    374214\n",
            "4.0    374214\n",
            "3.0    374214\n",
            "2.0    374214\n",
            "dtype: int64\n",
            "Shape of training data:  (1871070, 790226)\n",
            "Shape of test data:  (62323, 790226)\n",
            "Iteration  2\n",
            "(560904, 789900)\n",
            "(560904,)\n",
            "Number of observations in each class before oversampling (training data): \n",
            " 4.0    374124\n",
            "1.0     85736\n",
            "3.0     49659\n",
            "2.0     37962\n",
            "5.0     13423\n",
            "Name: ethinicity, dtype: int64\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
            "  warnings.warn(msg, category=FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Number of observations in each class after oversampling (training data): \n",
            " 1.0    374124\n",
            "5.0    374124\n",
            "4.0    374124\n",
            "3.0    374124\n",
            "2.0    374124\n",
            "dtype: int64\n",
            "Shape of training data:  (1870620, 789900)\n",
            "Shape of test data:  (62323, 789900)\n",
            "Iteration  3\n",
            "(560904, 789837)\n",
            "(560904,)\n",
            "Number of observations in each class before oversampling (training data): \n",
            " 4.0    374394\n",
            "1.0     85395\n",
            "3.0     49696\n",
            "2.0     37972\n",
            "5.0     13447\n",
            "Name: ethinicity, dtype: int64\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
            "  warnings.warn(msg, category=FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Number of observations in each class after oversampling (training data): \n",
            " 1.0    374394\n",
            "5.0    374394\n",
            "4.0    374394\n",
            "3.0    374394\n",
            "2.0    374394\n",
            "dtype: int64\n",
            "Shape of training data:  (1871970, 789837)\n",
            "Shape of test data:  (62323, 789837)\n",
            "Iteration  4\n",
            "(560904, 790116)\n",
            "(560904,)\n",
            "Number of observations in each class before oversampling (training data): \n",
            " 4.0    374340\n",
            "1.0     85493\n",
            "3.0     49660\n",
            "2.0     37975\n",
            "5.0     13436\n",
            "Name: ethinicity, dtype: int64\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
            "  warnings.warn(msg, category=FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Number of observations in each class after oversampling (training data): \n",
            " 1.0    374340\n",
            "5.0    374340\n",
            "4.0    374340\n",
            "3.0    374340\n",
            "2.0    374340\n",
            "dtype: int64\n",
            "Shape of training data:  (1871700, 790116)\n",
            "Shape of test data:  (62323, 790116)\n",
            "Iteration  5\n",
            "(560904, 789934)\n",
            "(560904,)\n",
            "Number of observations in each class before oversampling (training data): \n",
            " 4.0    374330\n",
            "1.0     85423\n",
            "3.0     49762\n",
            "2.0     38017\n",
            "5.0     13372\n",
            "Name: ethinicity, dtype: int64\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
            "  warnings.warn(msg, category=FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Number of observations in each class after oversampling (training data): \n",
            " 1.0    374330\n",
            "5.0    374330\n",
            "4.0    374330\n",
            "3.0    374330\n",
            "2.0    374330\n",
            "dtype: int64\n",
            "Shape of training data:  (1871650, 789934)\n",
            "Shape of test data:  (62323, 789934)\n",
            "Iteration  6\n",
            "(560904, 789856)\n",
            "(560904,)\n",
            "Number of observations in each class before oversampling (training data): \n",
            " 4.0    374281\n",
            "1.0     85461\n",
            "3.0     49745\n",
            "2.0     37939\n",
            "5.0     13478\n",
            "Name: ethinicity, dtype: int64\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
            "  warnings.warn(msg, category=FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Number of observations in each class after oversampling (training data): \n",
            " 1.0    374281\n",
            "5.0    374281\n",
            "4.0    374281\n",
            "3.0    374281\n",
            "2.0    374281\n",
            "dtype: int64\n",
            "Shape of training data:  (1871405, 789856)\n",
            "Shape of test data:  (62323, 789856)\n",
            "Iteration  7\n",
            "(560904, 790034)\n",
            "(560904,)\n",
            "Number of observations in each class before oversampling (training data): \n",
            " 4.0    374345\n",
            "1.0     85521\n",
            "3.0     49681\n",
            "2.0     37927\n",
            "5.0     13430\n",
            "Name: ethinicity, dtype: int64\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
            "  warnings.warn(msg, category=FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Number of observations in each class after oversampling (training data): \n",
            " 1.0    374345\n",
            "5.0    374345\n",
            "4.0    374345\n",
            "3.0    374345\n",
            "2.0    374345\n",
            "dtype: int64\n",
            "Shape of training data:  (1871725, 790034)\n",
            "Shape of test data:  (62323, 790034)\n",
            "Iteration  8\n",
            "(560905, 790009)\n",
            "(560905,)\n",
            "Number of observations in each class before oversampling (training data): \n",
            " 4.0    374298\n",
            "1.0     85489\n",
            "3.0     49693\n",
            "2.0     37965\n",
            "5.0     13460\n",
            "Name: ethinicity, dtype: int64\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
            "  warnings.warn(msg, category=FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Number of observations in each class after oversampling (training data): \n",
            " 1.0    374298\n",
            "5.0    374298\n",
            "4.0    374298\n",
            "3.0    374298\n",
            "2.0    374298\n",
            "dtype: int64\n",
            "Shape of training data:  (1871490, 790009)\n",
            "Shape of test data:  (62322, 790009)\n",
            "Iteration  9\n",
            "(560905, 789874)\n",
            "(560905,)\n",
            "Number of observations in each class before oversampling (training data): \n",
            " 4.0    374346\n",
            "1.0     85460\n",
            "3.0     49720\n",
            "2.0     37950\n",
            "5.0     13429\n",
            "Name: ethinicity, dtype: int64\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
            "  warnings.warn(msg, category=FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Number of observations in each class after oversampling (training data): \n",
            " 1.0    374346\n",
            "5.0    374346\n",
            "4.0    374346\n",
            "3.0    374346\n",
            "2.0    374346\n",
            "dtype: int64\n",
            "Shape of training data:  (1871730, 789874)\n",
            "Shape of test data:  (62322, 789874)\n",
            "Iteration  10\n",
            "(560905, 790302)\n",
            "(560905,)\n",
            "Number of observations in each class before oversampling (training data): \n",
            " 4.0    374248\n",
            "1.0     85486\n",
            "3.0     49715\n",
            "2.0     37975\n",
            "5.0     13481\n",
            "Name: ethinicity, dtype: int64\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
            "  warnings.warn(msg, category=FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Number of observations in each class after oversampling (training data): \n",
            " 1.0    374248\n",
            "5.0    374248\n",
            "4.0    374248\n",
            "3.0    374248\n",
            "2.0    374248\n",
            "dtype: int64\n",
            "Shape of training data:  (1871240, 790302)\n",
            "Shape of test data:  (62322, 790302)\n",
            "List of cross-validation accuracies for NBC:  [0.8784236959068081, 0.8736902909038397, 0.8794506041108419, 0.8775893329910306, 0.8769314699228214, 0.8793703769074017, 0.8792099225005214, 0.8768492667115946, 0.8774429575430827, 0.8793042585282885]\n",
            "Mean cross-validation accuracy for NBC:  0.877826217602623\n",
            "Best cross-validation accuracy for NBC:  0.8794506041108419\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         1.0       0.98      0.92      0.95      9599\n",
            "         2.0       0.94      0.11      0.20      4204\n",
            "         3.0       0.89      0.63      0.74      5542\n",
            "         4.0       0.85      0.99      0.92     41486\n",
            "         5.0       0.99      0.53      0.69      1492\n",
            "\n",
            "    accuracy                           0.88     62323\n",
            "   macro avg       0.93      0.64      0.70     62323\n",
            "weighted avg       0.89      0.88      0.85     62323\n",
            "\n",
            "Shape of tfidf matrix for saved SVC Model:  (623227, 860086)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9SvR8hrkSz3k"
      },
      "source": [
        "Our model was created using vocabulary of 790302 words., it performs \n",
        "well on first name with the accuracy of 86% . The model with name as feature, works best on\n",
        "groups (Asian-Indian Subcontinent, Hispanic, White non-Hispanic). It was expected as the \n",
        "vocabulary for these classes was more. Black non – Hispanic and Asian – East Asians received \n",
        "a recall score of 0.11 and 0.53. For our main group i.e., Indian – Subcontinent origin, we got \n",
        "precision and recall scores of 0.98 and 0.92, respectively. This observation tells us that NBC \n",
        "might perform well for our class. However, using such a conditional model results in more \n",
        "false positive. Which will result in increasing manual work. The advantage of using Naïve \n",
        "Bayes classification is that its training time is significantly less compared to others. Also, it \n",
        "does not need high computer memory and CPU consumption."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iR_CM4fmSdKH"
      },
      "source": [
        "## Deploying model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ug2RYQX3SgfR"
      },
      "source": [
        "\n",
        "To deploy this model on unseen data of the company, we import the data and the saved model and vocabulary. We cleaned the data and removed unwanted data that is not classified as human names (e.g., LLC, trust, ltd .etc). We also removed suffixes and special characters to get a better result. We ran the model on unseen data and checked the results.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HX83SsR5Stj9"
      },
      "source": [
        "#NBC MODEL DEPLOYMENT \n",
        "\n",
        "#importing libraries\n",
        "import re, nltk\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import csv\n",
        "from sklearn.externals import joblib\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "#reading compnay data , Data info, selecting relevant column\n",
        "df = pd.read_csv(\"main1.csv\")\n",
        "df.shape\n",
        "df.head()\n",
        "df =df[['prop_owner_name', 'IsSelectedforExport']]\n",
        "\n",
        "# convert the columns into lower case and remove na\n",
        "df['prop_owner_name'] = df['prop_owner_name'].str.lower()\n",
        "df = df.dropna()\n",
        " \n",
        "# removing unwanted data\n",
        "df = df[~(df.prop_owner_name.str.contains('llc'))]\n",
        "df = df[~(df.prop_owner_name.str.contains('ltd'))]\n",
        "df = df[~(df.prop_owner_name.str.contains('estate'))]\n",
        "df = df[~(df.prop_owner_name.str.contains('trust'))]\n",
        "df = df[~(df.prop_owner_name.str.contains('inc'))]\n",
        "df = df[~(df.prop_owner_name.str.contains('trustee'))]\n",
        "\n",
        "# spliting two names and saving them in new column\n",
        "new=df['prop_owner_name'].str.split(\"&\",n = 1, expand = True)\n",
        "df['name1']= new[0]\n",
        "\n",
        "# clean text, remove special charaters, numbers,suffix(Full name),  \n",
        "def cleaning(text):\n",
        "    only_words = re.sub('([^A-Za-z ]+)(^dr.)(^dr )(^mr.)(^mr )(^prof.)(^adv. )',' ',text )\n",
        "    return only_words\n",
        "df['name1']=df['name1'].apply(cleaning)\n",
        "\n",
        "#loading model and vocabulary\n",
        "model = joblib.load(\"nbc.sav\")\n",
        "vocabulary_model = pd.read_csv('ocabulary_NBC.csv')\n",
        "\n",
        "#converting vocabulary in dictonary\n",
        "vocabulary_model_dict = {}\n",
        "for i, word in enumerate(vocabulary_model['aa']):\n",
        "         vocabulary_model_dict[word] = i\n",
        "\n",
        "# countvectoriser\n",
        "countvect = CountVectorizer(ngram_range=(1,2),vocabulary = vocabulary_model_dict) \n",
        "name = countvect.fit_transform(data)\n",
        "a =countvect.get_feature_names()\n",
        "new_name_countvect= countvect.fit_transform(df['name1'])\n",
        "#prediting from model\n",
        "targets_pred = model.predict(new_name_countvect)\n",
        "df['predicited_ethnicity'] = targets_pred\n",
        "#saving prediciton\n",
        "df.to_csv('predicted_name_NBC.csv', encoding='utf-8', index=False)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tegIakuFSu4m"
      },
      "source": [
        ""
      ]
    }
  ]
}
