{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LSTM Single alphabet.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xJDylns-x8Py"
      },
      "source": [
        "** LSTM - Single alphabet model**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FsFLXZ_G0NF1"
      },
      "source": [
        "In this , we have created LSTM model with single alphabet as parameter. It uses short memory / forgets \n",
        "gates to retain patterns learned in sequence useful for predicting target variables. These kinds \n",
        "of networks are specialized in learning long term dependencies.They can \n",
        "remember the information for a long period of time. LSTM model is useful to predict the names \n",
        "which are not in the vocabulary as well. LSTM has this chain-like structure, but instead of \n",
        "having a single neural network layer, it has 4 layers interacting with each other In a very special \n",
        "way. LSTM take one full vector as input and pass-through all these layers doing a various task \n",
        "as addition, concatenation and copying. \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MmrJeuAw2G6_"
      },
      "source": [
        "# Import libraries necessary for this project\n",
        "from __future__ import print_function\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from keras.layers.core import Dense, Activation, Dropout\n",
        "from keras.preprocessing import sequence\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Embedding\n",
        "from keras.layers import LSTM\n",
        "from keras.datasets import imdb\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import re"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hsKPpJMM2zVY"
      },
      "source": [
        "# determining max lenth for charaters in name\n",
        "# after 30 it will cut the name\n",
        "# determing total number of lables\n",
        "maxlen = 30\n",
        "labels = 5"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WEh8ZcY91kXm"
      },
      "source": [
        "# Data Processing \n",
        "Here We import the ethnicity data from the file and select the relevant columns. We converted labels into numeric and clean the names by removing suffixes and special characters. Cleaning data is essential as it can create wrong models."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YzMRxSlz28Yi",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "cdba72e5-8da8-4770-f179-46c669311ce3"
      },
      "source": [
        "#reading data and info\n",
        "df = pd.read_csv(\"ethnicity collection data11-asian specific.csv\")\n",
        "df.shape\n",
        "df.head()\n",
        "df.groupby('ethinicity')['full name'].size()\n",
        "df =df[['full name','ethinicity']]\n",
        "\n",
        "# convert the columns into lower case and remove na\n",
        "# converting lables into numbers\n",
        "df['full name'] = df['full name'].str.lower()\n",
        "df['ethinicity']=df['ethinicity'].str.lower()\n",
        "df['ethinicity']=df['ethinicity'].map({'asian-indian':1,'black non hispanic':2,'hispanic':3 , 'white non hispanic':4,'asian-east':5})\n",
        "df = df.dropna()\n",
        "\n",
        "# clean text, remove special charaters, numbers,suffix(Full name),  \n",
        "def cleaning(text):\n",
        "    only_words = re.sub('([^A-Za-z ]+)|(^dr\\.)|(^dr )|(^mr\\.)|(^mr )|(^prof\\.)|(^adv\\. )',' ',text )\n",
        "    return only_words\n",
        "\n",
        "df['full_name_cleaned']=df['full name'].apply(cleaning)\n",
        "df.groupby('ethinicity')['full name'].size()\n",
        "\n",
        "\n",
        "# saving data to new file\n",
        "df.to_csv('data.csv', encoding='utf-8', index=False)\n",
        "#load new data\n",
        "cleaned_data = pd.read_csv(\"data.csv\", encoding = \"ISO-8859-1\")\n",
        "pd.set_option('display.max_colwidth', None)\n",
        "#convert lables \n",
        "data = cleaned_data.full_name_cleaned\n",
        "target = cleaned_data.ethinicity  "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py:2718: DtypeWarning: Columns (0,1,2,5,7) have mixed types.Specify dtype option on import or set low_memory=False.\n",
            "  interactivity=interactivity, compiler=compiler, result=result)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GSIJy-lH3Q6o"
      },
      "source": [
        "# creating a vocab list \n",
        "vocab = set(' '.join([str(i) for i in data]))\n",
        "vocab.add('END')\n",
        "vocab.add('-')\n",
        "len_vocab = len(vocab)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6DPIObgb3WeY",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "14e80473-63d7-4d3c-9b5f-7dd2847a8c48"
      },
      "source": [
        "print(vocab)\n",
        "print(\"vocab length is \",len_vocab)\n",
        "print (\"length of input is \",len(cleaned_data))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'w', 'c', 'l', 'a', 'END', 'y', 't', 'd', '-', 'r', 'i', 'u', 'h', 'e', 'm', 'q', 'p', 'z', 'b', 'n', 'o', ' ', 'k', 'v', 'f', 'x', 'j', 's', 'g'}\n",
            "vocab length is  29\n",
            "length of input is  623227\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X4VsFm3Q3kA3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "71e6b048-7a94-461b-b462-c3fba1a41281"
      },
      "source": [
        "# converting vocab into dictionary\n",
        "char_index = dict((c, i) for i, c in enumerate(vocab))\n",
        "print(char_index)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'w': 0, 'c': 1, 'l': 2, 'a': 3, 'END': 4, 'y': 5, 't': 6, 'd': 7, '-': 8, 'r': 9, 'i': 10, 'u': 11, 'h': 12, 'e': 13, 'm': 14, 'q': 15, 'p': 16, 'z': 17, 'b': 18, 'n': 19, 'o': 20, ' ': 21, 'k': 22, 'v': 23, 'f': 24, 'x': 25, 'j': 26, 's': 27, 'g': 28}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XWjsvzl_49ln"
      },
      "source": [
        "#train test split\n",
        "msk = np.random.rand(len(cleaned_data)) < 0.8\n",
        "train = cleaned_data[msk]\n",
        "test = cleaned_data[~msk]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ywYl7ZYW7UQw"
      },
      "source": [
        "def set_flag(i):\n",
        "    tmp = np.zeros(29);\n",
        "    tmp[i] = 1\n",
        "    return(tmp)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dCKV4beA7r0_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "2117f3a6-ed9a-4fcf-ae26-275faab32275"
      },
      "source": [
        "set_flag(3)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CvPpBlTM8NH_"
      },
      "source": [
        "#encode to vector space(one hot encoding)\n",
        "#padd 'END' to shorter sequences\n",
        "#also convert each index to one-hot encoding\n",
        "train_X = []\n",
        "train_Y = []\n",
        "trunc_train_name = [str(i)[0:maxlen] for i in train['full_name_cleaned']]\n",
        "for i in trunc_train_name:\n",
        "    tmp = [set_flag(char_index[j]) for j in str(i)]\n",
        "    for k in range(0,maxlen - len(str(i))):\n",
        "        tmp.append(set_flag(char_index[\"END\"]))\n",
        "    train_X.append(tmp)\n",
        "#converting test data labale into vecotrs mannualy.\n",
        "for i in train['ethinicity']:\n",
        "    if i == 1:\n",
        "        train_Y.append([1,0,0,0,0])\n",
        "    elif i == 2 :\n",
        "        train_Y.append([0,1,0,0,0])\n",
        "    elif i == 3 :\n",
        "        train_Y.append([0,0,1,0,0])\n",
        "    elif i == 4 : \n",
        "        train_Y.append([0,0,0,1,0])\n",
        "    elif i == 5 :\n",
        "        train_Y.append([0,0,0,0,1])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tvDb901O2_va"
      },
      "source": [
        "#Model creation\n",
        "The first step in LSTM is to decide which information is not useful and throw away \n",
        "from the cell state. The decision is made by the sigmoid layer , it looks at the input (ð»ð‘¡ âˆ’\n",
        "1 ð‘Žð‘›ð‘‘ ð‘‹ð‘¡) and assign a number between 0 and 1 as a output to cell state. In this case, 0 indicates\n",
        "that cell needs to be completely forgotten this while 1 represent completely keep this.Once this \n",
        "20\n",
        "is done the vector is pass to the second layer where what new information to be stored in cell \n",
        "is decided .This part is divided into two part:\n",
        "\n",
        "1) A layer to decided which values to be updated\n",
        "\n",
        "2)A ð‘¡ð‘Žâ„Ž layer, which creates a new vector for new candidate values(ð¶ð‘¡). \n",
        "\n",
        "In the end, both were combined to update the state of the cell (ð¶ð‘¡). In this step, the model does \n",
        "what it has decided to do in the first step. It forgets what It has decided to forget and update the \n",
        "new state of the cell by the scale decided. In the end, the model decides the output it is going \n",
        "to generate. The output is based on the cell state, which was updated in the previous step. The \n",
        "sigmoid layer is again used here to decide what part of the cell state will be output. After this,\n",
        "the cell state passes through and multiply it by the output of the sigmoid gate. With our text \n",
        "classification problem, we input a name as a sequence of alphabets or bi grams related to each \n",
        "other in some way. We make predictions at the end of the article when the model is fed with \n",
        "all the name's alphabets. The input is the sequence of the alphabet or bi grams, and output is \n",
        "one label. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wd7-7pAR6IHD"
      },
      "source": [
        "For the purposes of this project, the following steps have been used for our model :\n",
        "\n",
        "\n",
        "1.   We used a stacked LSTM model with 2 LSTM layers for only the characters model\n",
        "2.    We used the final dense layer with SoftMax activation\n",
        "3. Used many to one architecture\n",
        "4. Cross â€“ entropy loss is used with \"Adam\" Optimizer\n",
        "5. To avoid the problem of overfitting, we also used 0.2 dropouts and 0.2 recurrent dropouts\n",
        "6. Model was fir with 10 epochs with a batch size of 100. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P2ILBGLj_KwU",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "outputId": "38f6f6ec-178d-4944-b799-15bb7d9b5d5a"
      },
      "source": [
        "#build the model: 2 stacked LSTM\n",
        "print('Build model...')\n",
        "model = Sequential()\n",
        "model.add(LSTM(128, return_sequences=True, input_shape=(maxlen,len_vocab)))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(LSTM(128, return_sequences=False))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(5))\n",
        "model.add(Activation('softmax'))\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam',metrics=['accuracy'])\n",
        "\n",
        "print(model.summary())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Build model...\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "lstm (LSTM)                  (None, 30, 128)           80896     \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 30, 128)           0         \n",
            "_________________________________________________________________\n",
            "lstm_1 (LSTM)                (None, 128)               131584    \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 5)                 645       \n",
            "_________________________________________________________________\n",
            "activation (Activation)      (None, 5)                 0         \n",
            "=================================================================\n",
            "Total params: 213,125\n",
            "Trainable params: 213,125\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lctlF1tnA6Ij"
      },
      "source": [
        "#creating test data\n",
        "test_X = []\n",
        "test_Y = []\n",
        "trunc_test_name = [str(i)[0:maxlen] for i in test['full_name_cleaned']]\n",
        "for i in trunc_test_name:\n",
        "    tmp = [set_flag(char_index[j]) for j in str(i)]\n",
        "    for k in range(0,maxlen - len(str(i))):\n",
        "        tmp.append(set_flag(char_index[\"END\"]))\n",
        "    test_X.append(tmp)\n",
        "#converting test data labale into vecotrs mannualy.\n",
        "for i in test['ethinicity']:\n",
        "  if i == 1.0:\n",
        "    test_Y.append([1,0,0,0,0])\n",
        "  elif i == 2.0:\n",
        "    test_Y.append([0,1,0,0,0])\n",
        "  elif i == 3.0:\n",
        "    test_Y.append([0,0,1,0,0])\n",
        "  elif i == 4.0: \n",
        "    test_Y.append([0,0,0,1,0])\n",
        "  elif i == 5.0:\n",
        "    test_Y.append([0,0,0,0,1])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eJV7L9qPBIwu",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "a6a7420f-511a-42e0-b45f-b566c0082d1b"
      },
      "source": [
        "print(np.asarray(test_X).shape)\n",
        "print(np.asarray(test_Y).shape)\n",
        "test_X = np.array(test_X)\n",
        "test_Y =np.array(test_Y)\n",
        "train_X = np.array(train_X)\n",
        "train_Y = np.array(train_Y)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(124753, 30, 29)\n",
            "(124753, 5)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bbp-VOJ4DOl4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 394
        },
        "outputId": "f362f57b-59ef-4fc9-af2d-76067c352214"
      },
      "source": [
        "#model fitting\n",
        "batch_size=100\n",
        "model.fit(train_X, train_Y,batch_size=batch_size,epochs=10,validation_data=(test_X, test_Y))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "4985/4985 [==============================] - 882s 177ms/step - loss: 0.4883 - accuracy: 0.8452 - val_loss: 0.4061 - val_accuracy: 0.8713\n",
            "Epoch 2/10\n",
            "4985/4985 [==============================] - 875s 176ms/step - loss: 0.3822 - accuracy: 0.8783 - val_loss: 0.3505 - val_accuracy: 0.8884\n",
            "Epoch 3/10\n",
            "4985/4985 [==============================] - 871s 175ms/step - loss: 0.3485 - accuracy: 0.8890 - val_loss: 0.3284 - val_accuracy: 0.8944\n",
            "Epoch 4/10\n",
            "4985/4985 [==============================] - 870s 175ms/step - loss: 0.3273 - accuracy: 0.8955 - val_loss: 0.3155 - val_accuracy: 0.8984\n",
            "Epoch 5/10\n",
            "4985/4985 [==============================] - 876s 176ms/step - loss: 0.3120 - accuracy: 0.9002 - val_loss: 0.3083 - val_accuracy: 0.9009\n",
            "Epoch 6/10\n",
            "4985/4985 [==============================] - 872s 175ms/step - loss: 0.3001 - accuracy: 0.9042 - val_loss: 0.3034 - val_accuracy: 0.9025\n",
            "Epoch 7/10\n",
            "4985/4985 [==============================] - 871s 175ms/step - loss: 0.2900 - accuracy: 0.9067 - val_loss: 0.3043 - val_accuracy: 0.9000\n",
            "Epoch 8/10\n",
            "4985/4985 [==============================] - 864s 173ms/step - loss: 0.2813 - accuracy: 0.9098 - val_loss: 0.2962 - val_accuracy: 0.9054\n",
            "Epoch 9/10\n",
            "4985/4985 [==============================] - 863s 173ms/step - loss: 0.2737 - accuracy: 0.9123 - val_loss: 0.2932 - val_accuracy: 0.9065\n",
            "Epoch 10/10\n",
            "4985/4985 [==============================] - 858s 172ms/step - loss: 0.2667 - accuracy: 0.9143 - val_loss: 0.2927 - val_accuracy: 0.9059\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7ff37d8f45c0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zaqRPMC2pcKB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "f049016d-ea18-43c8-86d4-7d7984e7c317"
      },
      "source": [
        "#model score\n",
        "score, acc = model.evaluate(test_X, test_Y)\n",
        "print('Test score:', score)\n",
        "print('Test accuracy:', acc)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "3899/3899 [==============================] - 76s 20ms/step - loss: 0.2927 - accuracy: 0.9059\n",
            "Test score: 0.29269641637802124\n",
            "Test accuracy: 0.9059421420097351\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PEPd6iXYq79H"
      },
      "source": [
        "\n",
        "evals = model.predict_classes(test_X)\n",
        "prob_m = [i[0] for i in evals]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "spondMTmr4GI"
      },
      "source": [
        "y_pred = model.predict_classes(test_X, verbose=2)\n",
        "p = model.predict_proba(test_X, verbose=2) # to predict probability"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uV0v41IDue4U"
      },
      "source": [
        "target_names= ['asian-indian','black non hispanic','hispanic' , 'white non hispanic','asian-east']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GF6TGPIatHpo",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        },
        "outputId": "8fde3416-4e47-4332-a886-d848f231769d"
      },
      "source": [
        "from sklearn.metrics import classification_report,confusion_matrix,accuracy_score\n",
        "print(classification_report(np.argmax(test_Y, axis=1), y_pred, target_names=target_names))\n",
        "print(confusion_matrix(np.argmax(test_Y, axis=1), y_pred))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                    precision    recall  f1-score   support\n",
            "\n",
            "      asian-indian       0.98      0.98      0.98     19174\n",
            "black non hispanic       0.66      0.39      0.49      8337\n",
            "          hispanic       0.80      0.77      0.79     10861\n",
            "white non hispanic       0.92      0.96      0.94     83334\n",
            "        asian-east       0.91      0.87      0.89      3047\n",
            "\n",
            "          accuracy                           0.91    124753\n",
            "         macro avg       0.85      0.79      0.82    124753\n",
            "      weighted avg       0.90      0.91      0.90    124753\n",
            "\n",
            "[[18807    44    62   217    44]\n",
            " [   61  3251   215  4798    12]\n",
            " [   97   202  8340  2112   110]\n",
            " [  211  1393  1668 79979    83]\n",
            " [   72    34    93   206  2642]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IxSLZhCv7i2_"
      },
      "source": [
        "All the LSTM model works equally on the dataset. When we \n",
        "used bi -gram model on our data, the precision and recall score for the full name model was \n",
        "0.83 and 0.84. In comparison, single alphabet model was 0.90 and 0.90, respectively. When models were \n",
        "compared, we observed that the average recall score of all groups except black non-Hispanic \n",
        "is high. This tells us that , Black non-Hispanic people are challenging to predict. The cause of this might be less data for \n",
        "that class or not specific pattern in names. However, here we focus on the 'Asian- Indian \n",
        "Subcontinent\" group. We realized that score for this specific ethnicity was between 0.96 -0.98 \n",
        "across three models.Choosing \n",
        "the final model was difficult. We decided to go with LSTM â€“ Single alphabet model causes the \n",
        "accuracy and other ethnic groups' recall scores better than other models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UE66BCCqtzpZ"
      },
      "source": [
        "model.save('singlealphabet_lstm.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ihAiOMH8vnyR"
      },
      "source": [
        "import csv\n",
        "with open('singlealphabet_lstm_vocab.csv', 'w') as csv_file:  \n",
        "    writer = csv.writer(csv_file)\n",
        "    for key, value in char_index.items():\n",
        "       writer.writerow([key, value])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e5-qsoaO8UkQ"
      },
      "source": [
        "#Deploying model\n",
        "\n",
        "Deploying the model in real-world data is the final part of the project. Deploying requires cleaning the data, importing vocabulary and models, and processing the data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T-P4EQ2L88Yx"
      },
      "source": [
        "# LSTM SINGLE ALPHABET MODEL DEPOLYMENT\n",
        "\n",
        "import re, nltk\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import csv\n",
        "from keras.models import load_model\n",
        "\n",
        "#loading compnay data , info and selecting valid column\n",
        "df = pd.read_csv(\"county.csv\")\n",
        "df.shape\n",
        "df.head()\n",
        "df =df[['prop_owner_name', 'IsSelectedforExport']]\n",
        "\n",
        "# convert the columns into lower case and remove na\n",
        "df['prop_owner_name'] = df['prop_owner_name'].str.lower()\n",
        "df = df.dropna()\n",
        "\n",
        "#cleaning unwanted data\n",
        "df = df[~(df.prop_owner_name.str.contains('llc'))]\n",
        "df = df[~(df.prop_owner_name.str.contains('ltd'))]\n",
        "df = df[~(df.prop_owner_name.str.contains('estate'))]\n",
        "df = df[~(df.prop_owner_name.str.contains('trust'))]\n",
        "df = df[~(df.prop_owner_name.str.contains('inc'))]\n",
        "df = df[~(df.prop_owner_name.str.contains('trustee'))]\n",
        "\n",
        "new=df['prop_owner_name'].str.split(\"&\",n = 1, expand = True)\n",
        "df['name1']= new[0]\n",
        "\n",
        "# clean text, remove special charaters, numbers,suffix(Full name),  \n",
        "def cleaning(text):\n",
        "    only_words = re.sub('([^A-Za-z ]+)|(^dr.)|(^dr )|(^mr.)|(^mr )|(^prof.)|(^adv. )',' ',text )\n",
        "    return only_words\n",
        "df['name1']=df['name1'].apply(cleaning)\n",
        "\n",
        "# loading model\n",
        "model = load_model('singlealphabet_lstm.h5')\n",
        "vocab = pd.read_csv(\"singlealphabet_lstm_vocab.csv\")\n",
        "\n",
        "# converting vocab into dictonary\n",
        "char_index= {}\n",
        "for i, word in enumerate(vocab['vacab']):\n",
        "         char_index[word] = i\n",
        "# setting vector size to vocab size\n",
        "def set_flag(i):\n",
        "    tmp = np.zeros(29);\n",
        "    tmp[i] = 1\n",
        "    return(tmp)\n",
        "\n",
        "#converting names\n",
        "name=df['name1'].to_list()\n",
        "maxlen = 30\n",
        "labels = 5\n",
        "X=[]\n",
        "trunc_name = [i[0:maxlen] for i in name]\n",
        "for i in trunc_name:\n",
        "    tmp = [set_flag(char_index[j]) for j in str(i)]\n",
        "    for k in range(0,maxlen - len(str(i))):\n",
        "        tmp.append(set_flag(char_index[\"END\"]))\n",
        "    X.append(tmp)\n",
        "\n",
        "# predicitng using model\n",
        "pred=model.predict(np.asarray(X))\n",
        "len(pred)\n",
        "\n",
        "# giving lables to predicited values\n",
        "labels = ['asian-indian','black non hispanic','hispanic', 'white non hispanic','asian-east']\n",
        "a = list(pred)\n",
        "\n",
        "# appending predicited lables to list\n",
        "b = []\n",
        "for i in a :\n",
        "    l =labels[np.argmax(i)]\n",
        "    b.append(l)\n",
        " \n",
        "# saving predicited outcome\n",
        "df['prediciton']= b\n",
        "df.to_csv('lstm_predicted_ singleaphabet.csv', encoding='utf-8', index=False)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
